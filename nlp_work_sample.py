# -*- coding: utf-8 -*-
"""NLP work sample

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Nwy2_FmcswTaxP5UKrgbVvyyKB66y2WT
"""

import numpy as np
import pandas as pd
import io
from google.colab import drive

import matplotlib.pyplot as plt

import string
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import WhitespaceTokenizer

import matplotlib as mpl
from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator
from IPython.core.pylabtools import figsize
from PIL import Image

from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer
nltk.download("vader_lexicon")

# Load the Drive helper and mount
from google.colab import drive
# This will prompt for authorization.
drive.mount('/content/drive')

"""# Data Import and preparation"""

case = pd.read_csv("/content/drive/MyDrive/0. Project and Research/9. WAF - McDonld/Data/case_df.csv")
case = case[['id','caseid','date','contacttype','loyaltycase','customercomments']]

# Create date
case[['year','month','date2']] = case['date'].str.split('-',expand=True)
case['year-month']=case[['year','month']].agg('-'.join,axis=1)

# add weekday info
case['weekday'] = pd.to_datetime(case['date'])
case['dayname']=case['weekday'].dt.day_name() # monday
case['dayofweek']=case['weekday'].dt.day_of_week # numeric: monday=0
case=case.drop(['year','month','weekday'],axis=1)
case['year-month'].unique()


# Create 2021-12 as sample.
case = case[case['year-month']=='2021-12']
case.shape

"""# Pre-text cleaning"""

# Remove punctuation
def remove_punctuation(text):
    no_punct=[words for words in text if words not in string.punctuation]
    words_wo_punct=''.join(no_punct)
    words_wo_punct = words_wo_punct.rstrip(words_wo_punct[-1])
    return words_wo_punct

case['customercomments']=case['customercomments'].apply(lambda x: remove_punctuation(x))

# Remove digit 
def remove_digit(text):
  no_digit=[words for words in text if not words.isdigit()]
  words_wo_digit = ''.join(no_digit).strip().replace('  ',' ')
  return words_wo_digit

case['customercomments']=case['customercomments'].apply(lambda x: remove_digit(x))

# remove useless record start with ''' please refer to other case '''
case['drop'] = case['customercomments'].str.contains('Please refer to')
case = case[case['drop']==False]

case.shape

"""# Sentiment Analysis"""

#=============================== add sentiment score and analysis result ===========
vadersia = SentimentIntensityAnalyzer()
case['compound']=[vadersia.polarity_scores(x)['compound'] for x in case['customercomments']]


#============================= set threshold =========================
def get_analysis(score):
  if score <= -0.05:
    return 'Negative'
  elif score > 0.05:
    return "Positive"
  else: 
    return "Neutral"

#========================== get compound score and analysis of comments ==================
case['VaderAnalysis'] = case['compound'].apply(get_analysis)

"""# Keywords Extraction with BERT"""

from tqdm import tqdm
import spacy
from keybert import KeyBERT

kw_model = KeyBERT()
pos_model = spacy.load("en_core_web_sm")

def keywords_bert(comment):
    return kw_model.extract_keywords(comment, keyphrase_ngram_range=(1, 3), 
                                     stop_words='english',
                                     use_mmr=True, diversity=0.7)

def keywords_bert_pos(comment):
    # apply bert model and rank tokens
    ranked_candidates = kw_model.extract_keywords(comment, top_n = 9999, 
                                                  stop_words='english', use_mmr=True, diversity=0.7)

    # filter nouns and adjectives
    nouns = []
    adj = []
    for (word, score) in ranked_candidates:
        if pos_model(word)[0].pos_ in ['NOUN', 'PROPN']:
            nouns.append((word, score))
        elif pos_model(word)[0].pos_ in ['ADJ']:
            adj.append((word, score))

    # return top three for both
    result = []
    result.append(nouns[:min(3, len(nouns))])
    result.append(adj[:min(3, len(adj))])
    return result

tqdm.pandas()

case['keyphrases_list'] = case['customercomments'].progress_map(keywords_bert)

case['keywords_list_pos'] = case['customercomments'].progress_map(keywords_bert_pos)

"""# Topic modeling"""

# Tokenize for topic modeling
import re
def tokenize(text):
    words_split=re.split("\W+",text) 
    return words_split
case['clned_comments']=case['customercomments'].apply(lambda x: tokenize(x.lower()))


# Remove stopwords (words that are unnecessary) for topic modeling

nltk.download('stopwords')
stop_words = stopwords.words('english')
stop_words.extend(['customer', 'mcdonalds','mcdonald','n','said', 'get', 'told', 'didnt','wrote','go', 'please', 'got',
                   'im','ive','global','advised','yes','refer','wont'])

def remove_stopwords(text):
  words_wo_stopwords = [words for words in text if words not in stop_words]
  return words_wo_stopwords

case['clned_comments'] = case['clned_comments'].apply(lambda x: remove_stopwords(x))

"""# EDA"""

vader_overall = case['VaderAnalysis'].value_counts().plot.bar(color='lightblue')
plt.title("Count of Sentiment")
plt.ylabel("Count")
plt.xlabel("Sentiment Type")
plt.xticks(rotation = 0)
plt.show()

# compound score monthly summary 
vader_month_summary = case['compound'].groupby(case['dayofweek']).mean()
plt.plot(vader_month_summary)
plt.title("Sentiment Score & Day of Week")
plt.ylabel("Sentiment Score")
plt.xlabel("Day of Week")


plt.show()

case_email=case[case['contacttype']=='800EMAIL']
case_call=case[case['contacttype']=='800CALL']
case_neg = case[case['VaderAnalysis']=='negative']
case_pos = case[case['VaderAnalysis']=='positive']

case_neg_email=case_neg[case_neg['contacttype']=='800EMAIL']
case_neg_call=case_neg[case_neg['contacttype']=='800CALL']


case_email_call = case[case['contacttype'].isin(['800CALL','800EMAIL'])]

case_email_call['VaderAnalysis'].groupby(case['contacttype']).value_counts().unstack().plot.bar()

plt.title("Sentiment Analysis Count & Contact Type: Email vs. Call")
plt.ylabel("Count")
plt.xlabel("Contact Type")

plt.show()